{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17bf0b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 1s 2ms/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 3ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adagrad, Adadelta, Adamax, Nadam\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('water_potability.csv')\n",
    "\n",
    "# Calculate the count of missing values for each column\n",
    "missing_counts = data.isnull().sum()\n",
    "\n",
    "# Select the indices where the missing counts are zero\n",
    "zero_missing_indices = missing_counts.index[missing_counts.values == 0]\n",
    "\n",
    "# Select the indices where the missing counts are non-zero\n",
    "non_zero_missing_indices = missing_counts.index[missing_counts.values > 0]\n",
    "\n",
    "# Create a new dataset with only the columns having no missing values\n",
    "data_filled = data[zero_missing_indices]\n",
    "\n",
    "# Create a new dataset with only the columns having missing values\n",
    "data_skipped = data[non_zero_missing_indices]\n",
    "\n",
    "# Add the first column from data_skipped to data_filled\n",
    "data1 = pd.concat([data_filled, data_skipped.iloc[:, 0:1]], axis=1)\n",
    "\n",
    "# Select rows from data1 where the 'ph' column is not NaN\n",
    "data1_filled = data1[data1['ph'].notna()]\n",
    "\n",
    "# Split data into features and target\n",
    "X = data1_filled.drop(['ph'], axis=1)\n",
    "y = data1_filled['ph']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "search_space = {\n",
    "    'num_layers': np.arange(1, 20),  # Number of layers\n",
    "    'neurons_per_layer': np.arange(1, 128, 1),  # Neurons per layer\n",
    "    'activation_functions': ['relu', 'sigmoid', 'tanh', 'softmax', 'softplus',\n",
    "                             'softsign', 'elu', 'selu', 'gelu', 'hard_sigmoid', 'linear'],  # Activation functions\n",
    "    'optimizers': ['adam', 'sgd', 'rmsprop', 'adagrad', 'adadelta', 'adamax', 'nadam'],  # Optimizers\n",
    "    'alphas': [0.0001, 0.001, 0.01, 0.1],  # Learning rates\n",
    "    'loss_functions': ['mean_squared_error', 'mean_absolute_error',  'logcosh', 'huber_loss']\n",
    "}\n",
    "\n",
    "# Function to get optimizer object\n",
    "def get_optimizer(name, alpha):\n",
    "    optimizers = {\n",
    "        'adam': Adam(learning_rate=alpha),\n",
    "        'sgd': SGD(learning_rate=alpha),\n",
    "        'rmsprop': RMSprop(learning_rate=alpha),\n",
    "        'adagrad': Adagrad(learning_rate=alpha),\n",
    "        'adadelta': Adadelta(learning_rate=alpha),\n",
    "        'adamax': Adamax(learning_rate=alpha),\n",
    "        'nadam': Nadam(learning_rate=alpha)\n",
    "    }\n",
    "    return optimizers[name]\n",
    "\n",
    "# Define the fitness function\n",
    "def evaluate_model(params):\n",
    "    num_layers = params['num_layers']\n",
    "    neurons_per_layer = params['neurons_per_layer']\n",
    "    activation_functions = params['activation_functions']\n",
    "    optimizer_name = params['optimizer']\n",
    "    alpha = params['alpha']\n",
    "    loss_function = params['loss_function']\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons_per_layer[0], activation=activation_functions[0], input_shape=(X_train.shape[1],)))\n",
    "    for i in range(1, num_layers):\n",
    "        model.add(Dense(neurons_per_layer[i], activation=activation_functions[i]))\n",
    "    model.add(Dense(1))  # Output layer\n",
    "    optimizer = get_optimizer(optimizer_name, alpha)\n",
    "    model.compile(optimizer=optimizer, loss=loss_function)\n",
    "    model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0)\n",
    "    predictions = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    return r2\n",
    "\n",
    "# Define the genetic algorithm operations\n",
    "def initialize_population(population_size):\n",
    "    population = []\n",
    "    for _ in range(population_size):\n",
    "        num_layers = np.random.choice(search_space['num_layers'])\n",
    "        neurons_per_layer = np.random.choice(search_space['neurons_per_layer'], num_layers).tolist()\n",
    "        activation_functions = np.random.choice(search_space['activation_functions'], num_layers).tolist()\n",
    "        optimizer = np.random.choice(search_space['optimizers'])\n",
    "        alpha = np.random.choice(search_space['alphas'])\n",
    "        loss_function = np.random.choice(search_space['loss_functions'])\n",
    "        chromosome = {\n",
    "            'num_layers': num_layers,\n",
    "            'neurons_per_layer': neurons_per_layer,\n",
    "            'activation_functions': activation_functions,\n",
    "            'optimizer': optimizer,\n",
    "            'alpha': alpha,\n",
    "            'loss_function': loss_function\n",
    "        }\n",
    "        population.append(chromosome)\n",
    "    return population\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    child = {}\n",
    "    child['num_layers'] = np.random.choice([parent1['num_layers'], parent2['num_layers']])\n",
    "    child['neurons_per_layer'] = [np.random.choice([p1, p2]) for p1, p2 in zip(parent1['neurons_per_layer'], parent2['neurons_per_layer'])]\n",
    "    child['activation_functions'] = [np.random.choice([p1, p2]) for p1, p2 in zip(parent1['activation_functions'], parent2['activation_functions'])]\n",
    "    child['optimizer'] = np.random.choice([parent1['optimizer'], parent2['optimizer']])\n",
    "    child['alpha'] = np.random.choice([parent1['alpha'], parent2['alpha']])\n",
    "    child['loss_function'] = np.random.choice([parent1['loss_function'], parent2['loss_function']])\n",
    "    # Adjust the lists to the number of layers\n",
    "    child['neurons_per_layer'] = child['neurons_per_layer'][:child['num_layers']]\n",
    "    child['activation_functions'] = child['activation_functions'][:child['num_layers']]\n",
    "    return child\n",
    "\n",
    "def mutate(chromosome):\n",
    "    key_to_mutate = np.random.choice(['num_layers', 'neurons_per_layer', 'activation_functions', 'optimizer', 'alpha', 'loss_function'])\n",
    "    if key_to_mutate == 'num_layers':\n",
    "        chromosome[key_to_mutate] = np.random.choice(search_space['num_layers'])\n",
    "        chromosome['neurons_per_layer'] = np.random.choice(search_space['neurons_per_layer'], chromosome[key_to_mutate]).tolist()\n",
    "        chromosome['activation_functions'] = np.random.choice(search_space['activation_functions'], chromosome[key_to_mutate]).tolist()\n",
    "    elif key_to_mutate == 'neurons_per_layer':\n",
    "        layer_to_mutate = np.random.randint(0, chromosome['num_layers'])\n",
    "        chromosome[key_to_mutate][layer_to_mutate] = np.random.choice(search_space['neurons_per_layer'])\n",
    "    elif key_to_mutate == 'activation_functions':\n",
    "        layer_to_mutate = np.random.randint(0, chromosome['num_layers'])\n",
    "        chromosome[key_to_mutate][layer_to_mutate] = np.random.choice(search_space['activation_functions'])\n",
    "    elif key_to_mutate == 'optimizer':\n",
    "        chromosome[key_to_mutate] = np.random.choice(search_space['optimizers'])\n",
    "    elif key_to_mutate == 'alpha':\n",
    "        chromosome[key_to_mutate] = np.random.choice(search_space['alphas'])\n",
    "    elif key_to_mutate == 'loss_function':\n",
    "        chromosome[key_to_mutate] = np.random.choice(search_space['loss_functions'])\n",
    "    # Adjust the lists to the number of layers\n",
    "    chromosome['neurons_per_layer'] = chromosome['neurons_per_layer'][:chromosome['num_layers']]\n",
    "    chromosome['activation_functions'] = chromosome['activation_functions'][:chromosome['num_layers']]\n",
    "    return chromosome\n",
    "\n",
    "def select_parents(population, num_parents):\n",
    "    sorted_population = sorted(population, key=lambda x: x['fitness'], reverse=True)\n",
    "    return sorted_population[:num_parents]\n",
    "\n",
    "# Define the genetic algorithm parameters\n",
    "population_size = 4\n",
    "num_generations = 2\n",
    "num_parents_to_select = 2\n",
    "\n",
    "# Initialize the population\n",
    "population = initialize_population(population_size)\n",
    "\n",
    "# Store parameters and R2 scores on each iteration\n",
    "results = []\n",
    "\n",
    "# Main loop of the genetic algorithm\n",
    "for generation in range(num_generations):\n",
    "    # Evaluate the fitness of each individual in the population\n",
    "    for individual in population:\n",
    "        individual['fitness'] = evaluate_model(individual)\n",
    "    \n",
    "    # Store parameters and R2 scores\n",
    "    for individual in population:\n",
    "        result = {\n",
    "            'num_layers': individual['num_layers'],\n",
    "            'neurons_per_layer': individual['neurons_per_layer'],\n",
    "            'activation_functions': individual['activation_functions'],\n",
    "            'optimizer': individual['optimizer'],\n",
    "            'alpha': individual['alpha'],\n",
    "            'loss_function': individual['loss_function'],\n",
    "            'R2_score': individual['fitness']\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    # Select the best individuals as parents for the next generation\n",
    "    parents = select_parents(population, num_parents_to_select)\n",
    "    \n",
    "    # Generate offspring through crossover and mutation\n",
    "    offspring = []\n",
    "    while len(offspring) < population_size:\n",
    "        parent1 = np.random.choice(parents)\n",
    "        parent2 = np.random.choice(parents)\n",
    "        child = crossover(parent1, parent2)\n",
    "        if np.random.rand() < 0.1:  # Mutation probability\n",
    "            child = mutate(child)\n",
    "        offspring.append(child)\n",
    "    \n",
    "    # Replace the old population with the offspring\n",
    "    population = offspring\n",
    "\n",
    "# Store results in a CSV file\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('genetic_algorithm_results_5.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d86bdab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 3ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 1s 4ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 3ms/step\n",
      "18/18 [==============================] - 0s 3ms/step\n",
      "18/18 [==============================] - 0s 3ms/step\n",
      "18/18 [==============================] - 0s 3ms/step\n",
      "18/18 [==============================] - 0s 3ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 3ms/step\n",
      "18/18 [==============================] - 0s 3ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 3ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 3ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 3ms/step\n",
      "18/18 [==============================] - 0s 3ms/step\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "18/18 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adagrad, Adadelta, Adamax, Nadam\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('water_potability.csv')\n",
    "\n",
    "# Calculate the count of missing values for each column\n",
    "missing_counts = data.isnull().sum()\n",
    "\n",
    "# Select the indices where the missing counts are zero\n",
    "zero_missing_indices = missing_counts.index[missing_counts.values == 0]\n",
    "\n",
    "# Select the indices where the missing counts are non-zero\n",
    "non_zero_missing_indices = missing_counts.index[missing_counts.values > 0]\n",
    "\n",
    "# Create a new dataset with only the columns having no missing values\n",
    "data_filled = data[zero_missing_indices]\n",
    "\n",
    "# Create a new dataset with only the columns having missing values\n",
    "data_skipped = data[non_zero_missing_indices]\n",
    "\n",
    "# Add the first column from data_skipped to data_filled\n",
    "data1 = pd.concat([data_filled, data_skipped.iloc[:, 0:1]], axis=1)\n",
    "\n",
    "# Select rows from data1 where the 'ph' column is not NaN\n",
    "data1_filled = data1[data1['ph'].notna()]\n",
    "\n",
    "# Split data into features and target\n",
    "X = data1_filled.drop(['ph'], axis=1)\n",
    "y = data1_filled['ph']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "search_space = {\n",
    "    'num_layers': np.arange(1, 20),  # Number of layers\n",
    "    'neurons_per_layer': np.arange(1, 128, 1),  # Neurons per layer\n",
    "    'activation_functions': ['relu', 'sigmoid', 'tanh', 'softmax', 'softplus',\n",
    "                             'softsign', 'elu', 'selu', 'gelu', 'hard_sigmoid', 'linear'],  # Activation functions\n",
    "    'optimizers': ['adam', 'sgd', 'rmsprop', 'adagrad', 'adadelta', 'adamax', 'nadam'],  # Optimizers\n",
    "    'alphas': [0.0001, 0.001, 0.01, 0.1],  # Learning rates\n",
    "    'loss_functions': ['mean_squared_error', 'mean_absolute_error',  'logcosh', 'huber_loss']\n",
    "}\n",
    "\n",
    "# Function to get optimizer object\n",
    "def get_optimizer(name, alpha):\n",
    "    optimizers = {\n",
    "        'adam': Adam(learning_rate=alpha),\n",
    "        'sgd': SGD(learning_rate=alpha),\n",
    "        'rmsprop': RMSprop(learning_rate=alpha),\n",
    "        'adagrad': Adagrad(learning_rate=alpha),\n",
    "        'adadelta': Adadelta(learning_rate=alpha),\n",
    "        'adamax': Adamax(learning_rate=alpha),\n",
    "        'nadam': Nadam(learning_rate=alpha)\n",
    "    }\n",
    "    return optimizers[name]\n",
    "\n",
    "# Define the fitness function\n",
    "def evaluate_model(params, individual_id):\n",
    "    num_layers = params['num_layers']\n",
    "    neurons_per_layer = params['neurons_per_layer']\n",
    "    activation_functions = params['activation_functions']\n",
    "    optimizer_name = params['optimizer']\n",
    "    alpha = params['alpha']\n",
    "    loss_function = params['loss_function']\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons_per_layer[0], activation=activation_functions[0], input_shape=(X_train.shape[1],)))\n",
    "    for i in range(1, num_layers):\n",
    "        model.add(Dense(neurons_per_layer[i], activation=activation_functions[i]))\n",
    "    model.add(Dense(1))  # Output layer\n",
    "    optimizer = get_optimizer(optimizer_name, alpha)\n",
    "    model.compile(optimizer=optimizer, loss=loss_function)\n",
    "    \n",
    "    history = model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0)\n",
    "    predictions = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    \n",
    "    # Save parameters to a .txt file\n",
    "    with open(f'individuals/individual_{individual_id}_params.txt', 'w') as file:\n",
    "        file.write(f'num_layers: {num_layers}\\n')\n",
    "        file.write(f'neurons_per_layer: {neurons_per_layer}\\n')\n",
    "        file.write(f'activation_functions: {activation_functions}\\n')\n",
    "        file.write(f'optimizer: {optimizer_name}\\n')\n",
    "        file.write(f'alpha: {alpha}\\n')\n",
    "        file.write(f'loss_function: {loss_function}\\n')\n",
    "        file.write(f'r2_score: {r2}\\n')\n",
    "        \n",
    "    # Save loss history to a .txt file\n",
    "    with open(f'individuals/individual_{individual_id}_loss_history.txt', 'w') as file:\n",
    "        for loss in history.history['loss']:\n",
    "            file.write(f'{loss}\\n')\n",
    "    \n",
    "    # Save y_test and y_pred to a .txt file\n",
    "    predictions = predictions.flatten()\n",
    "    with open(f'individuals/individual_{individual_id}_predictions.txt', 'w') as file:\n",
    "        for true, pred in zip(y_test, predictions):\n",
    "            file.write(f'{true}, {pred}\\n')\n",
    "    \n",
    "    return r2\n",
    "\n",
    "# Define the genetic algorithm operations\n",
    "def initialize_population(population_size):\n",
    "    population = []\n",
    "    for _ in range(population_size):\n",
    "        num_layers = np.random.choice(search_space['num_layers'])\n",
    "        neurons_per_layer = np.random.choice(search_space['neurons_per_layer'], num_layers).tolist()\n",
    "        activation_functions = np.random.choice(search_space['activation_functions'], num_layers).tolist()\n",
    "        optimizer = np.random.choice(search_space['optimizers'])\n",
    "        alpha = np.random.choice(search_space['alphas'])\n",
    "        loss_function = np.random.choice(search_space['loss_functions'])\n",
    "        chromosome = {\n",
    "            'num_layers': num_layers,\n",
    "            'neurons_per_layer': neurons_per_layer,\n",
    "            'activation_functions': activation_functions,\n",
    "            'optimizer': optimizer,\n",
    "            'alpha': alpha,\n",
    "            'loss_function': loss_function\n",
    "        }\n",
    "        population.append(chromosome)\n",
    "    return population\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    child = {}\n",
    "    child['num_layers'] = np.random.choice([parent1['num_layers'], parent2['num_layers']])\n",
    "    child['neurons_per_layer'] = [np.random.choice([p1, p2]) for p1, p2 in zip(parent1['neurons_per_layer'], parent2['neurons_per_layer'])]\n",
    "    child['activation_functions'] = [np.random.choice([p1, p2]) for p1, p2 in zip(parent1['activation_functions'], parent2['activation_functions'])]\n",
    "    child['optimizer'] = np.random.choice([parent1['optimizer'], parent2['optimizer']])\n",
    "    child['alpha'] = np.random.choice([parent1['alpha'], parent2['alpha']])\n",
    "    child['loss_function'] = np.random.choice([parent1['loss_function'], parent2['loss_function']])\n",
    "    # Adjust the lists to the number of layers\n",
    "    child['neurons_per_layer'] = child['neurons_per_layer'][:child['num_layers']]\n",
    "    child['activation_functions'] = child['activation_functions'][:child['num_layers']]\n",
    "    return child\n",
    "\n",
    "def mutate(chromosome):\n",
    "    key_to_mutate = np.random.choice(['num_layers', 'neurons_per_layer', 'activation_functions', 'optimizer', 'alpha', 'loss_function'])\n",
    "    if key_to_mutate == 'num_layers':\n",
    "        chromosome[key_to_mutate] = np.random.choice(search_space['num_layers'])\n",
    "        chromosome['neurons_per_layer'] = np.random.choice(search_space['neurons_per_layer'], chromosome[key_to_mutate]).tolist()\n",
    "        chromosome['activation_functions'] = np.random.choice(search_space['activation_functions'], chromosome[key_to_mutate]).tolist()\n",
    "    elif key_to_mutate == 'neurons_per_layer':\n",
    "        layer_to_mutate = np.random.randint(0, chromosome['num_layers'])\n",
    "        chromosome[key_to_mutate][layer_to_mutate] = np.random.choice(search_space['neurons_per_layer'])\n",
    "    elif key_to_mutate == 'activation_functions':\n",
    "        layer_to_mutate = np.random.randint(0, chromosome['num_layers'])\n",
    "        chromosome[key_to_mutate][layer_to_mutate] = np.random.choice(search_space['activation_functions'])\n",
    "    elif key_to_mutate == 'optimizer':\n",
    "        chromosome[key_to_mutate] = np.random.choice(search_space['optimizers'])\n",
    "    elif key_to_mutate == 'alpha':\n",
    "        chromosome[key_to_mutate] = np.random.choice(search_space['alphas'])\n",
    "    elif key_to_mutate == 'loss_function':\n",
    "        chromosome[key_to_mutate] = np.random.choice(search_space['loss_functions'])\n",
    "    # Adjust the lists to the number of layers\n",
    "    chromosome['neurons_per_layer'] = chromosome['neurons_per_layer'][:chromosome['num_layers']]\n",
    "    chromosome['activation_functions'] = chromosome['activation_functions'][:chromosome['num_layers']]\n",
    "    return chromosome\n",
    "\n",
    "def select_parents(population, num_parents):\n",
    "    sorted_population = sorted(population, key=lambda x: x['fitness'], reverse=True)\n",
    "    return sorted_population[:num_parents]\n",
    "\n",
    "# Define the genetic algorithm parameters\n",
    "population_size = 100\n",
    "num_generations = 4\n",
    "num_parents_to_select = 10\n",
    "\n",
    "# Initialize the population\n",
    "population = initialize_population(population_size)\n",
    "\n",
    "# Store parameters and R2 scores on each iteration\n",
    "results = []\n",
    "\n",
    "# Main loop of the genetic algorithm\n",
    "for generation in range(num_generations):\n",
    "    # Evaluate the fitness of each individual in the population\n",
    "    for i, individual in enumerate(population):\n",
    "        individual['fitness'] = evaluate_model(individual, f'g{generation}_i{i}')\n",
    "    \n",
    "    # Store parameters and R2 scores\n",
    "    for individual in population:\n",
    "        result = {\n",
    "            'num_layers': individual['num_layers'],\n",
    "            'neurons_per_layer': individual['neurons_per_layer'],\n",
    "            'activation_functions': individual['activation_functions'],\n",
    "            'optimizer': individual['optimizer'],\n",
    "            'alpha': individual['alpha'],\n",
    "            'loss_function': individual['loss_function'],\n",
    "            'R2_score': individual['fitness']\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    # Select the best individuals as parents for the next generation\n",
    "    parents = select_parents(population, num_parents_to_select)\n",
    "    \n",
    "    # Generate offspring through crossover and mutation\n",
    "    offspring = []\n",
    "    while len(offspring) < population_size:\n",
    "        parent1 = np.random.choice(parents)\n",
    "        parent2 = np.random.choice(parents)\n",
    "        child = crossover(parent1, parent2)\n",
    "        if np.random.rand() < 0.1:  # Mutation probability\n",
    "            child = mutate(child)\n",
    "        offspring.append(child)\n",
    "    \n",
    "    # Replace the old population with the offspring\n",
    "    population = offspring\n",
    "\n",
    "# Store results in a CSV file\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('genetic_algorithm_results_6.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99709b6-cf43-48f6-ab2b-d6d1cf0e6a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import roc_auc_score, r2_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('water_potability.csv')\n",
    "\n",
    "# Calculate the count of missing values for each column\n",
    "missing_counts = data.isnull().sum()\n",
    "\n",
    "# Select the indices where the missing counts are zero\n",
    "zero_missing_indices = missing_counts.index[missing_counts.values == 0]\n",
    "\n",
    "# Select the indices where the missing counts are non-zero\n",
    "non_zero_missing_indices = missing_counts.index[missing_counts.values > 0]\n",
    "\n",
    "# Create a new dataset with only the columns having no missing values\n",
    "data_filled = data[zero_missing_indices]\n",
    "\n",
    "# Add the first column from data_skipped to data_filled\n",
    "data1 = pd.concat([data_filled, data_skipped.iloc[:, 0:1]], axis=1)\n",
    "\n",
    "# Select rows from data1 where the 'ph' column is not NaN\n",
    "data1_filled = data1[data1['ph'].notna()]\n",
    "\n",
    "# Split data into features and target\n",
    "X = data1_filled.drop(['ph'], axis=1)\n",
    "y = data1_filled['ph']\n",
    "# Split data into features and target\n",
    "X = data1_filled.drop(['ph'], axis=1)\n",
    "y = data1_filled['ph']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(18, activation='gelu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(97, activation='selu'))\n",
    "model.add(Dense(47, activation='softplus'))\n",
    "model.add(Dense(39, activation='relu'))\n",
    "model.add(Dense(54, activation='softplus'))\n",
    "model.add(Dense(9, activation='softsign'))\n",
    "model.add(Dense(55, activation='gelu'))\n",
    "model.add(Dense(104, activation='sigmoid'))\n",
    "model.add(Dense(42, activation='softsign'))\n",
    "model.add(Dense(37, activation='sigmoid'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=Adadelta(lr=0.01), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "# Calculate R2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"R2 Score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70981ab8-5673-4ec1-a2aa-cc117c7385fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adagrad, Adadelta, Adamax, Nadam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('water_potability.csv')\n",
    "\n",
    "# Calculate the count of missing values for each column\n",
    "missing_counts = data.isnull().sum()\n",
    "\n",
    "# Select the indices where the missing counts are zero\n",
    "zero_missing_indices = missing_counts.index[missing_counts.values == 0]\n",
    "\n",
    "# Select the indices where the missing counts are non-zero\n",
    "non_zero_missing_indices = missing_counts.index[missing_counts.values > 0]\n",
    "\n",
    "# Create a new dataset with only the columns having no missing values\n",
    "data_filled = data[zero_missing_indices]\n",
    "\n",
    "# Create a new dataset with only the columns having missing values\n",
    "data_skipped = data[non_zero_missing_indices]\n",
    "\n",
    "# Add the first column from data_skipped to data_filled\n",
    "data1 = pd.concat([data_filled, data_skipped.iloc[:, 0:1]], axis=1)\n",
    "\n",
    "# Select rows from data1 where the 'ph' column is not NaN\n",
    "data1_filled = data1[data1['ph'].notna()]\n",
    "\n",
    "# Split data into features and target\n",
    "X = data1_filled.drop(['ph'], axis=1)\n",
    "y = data1_filled['ph']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "search_space = {\n",
    "    'num_layers': np.arange(1, 20),  # Number of layers\n",
    "    'neurons_per_layer': np.arange(1, 128, 1),  # Neurons per layer\n",
    "    'activation_functions': ['relu', 'sigmoid', 'tanh', 'softmax', 'softplus',\n",
    "                             'softsign', 'elu', 'selu', 'gelu', 'hard_sigmoid', 'linear'],  # Activation functions\n",
    "    'optimizers': ['adam', 'sgd', 'rmsprop', 'adagrad', 'adadelta', 'adamax', 'nadam'],  # Optimizers\n",
    "    'alphas': [0.0001, 0.001, 0.01, 0.1],  # Learning rates\n",
    "    'loss_functions': ['mean_squared_error', 'mean_absolute_error',  'logcosh', 'huber_loss']\n",
    "}\n",
    "\n",
    "# Function to get optimizer object\n",
    "def get_optimizer(name, alpha):\n",
    "    optimizers = {\n",
    "        'adam': Adam(learning_rate=alpha),\n",
    "        'sgd': SGD(learning_rate=alpha),\n",
    "        'rmsprop': RMSprop(learning_rate=alpha),\n",
    "        'adagrad': Adagrad(learning_rate=alpha),\n",
    "        'adadelta': Adadelta(learning_rate=alpha),\n",
    "        'adamax': Adamax(learning_rate=alpha),\n",
    "        'nadam': Nadam(learning_rate=alpha)\n",
    "    }\n",
    "    return optimizers[name]\n",
    "\n",
    "# Define the fitness function\n",
    "def evaluate_model(params, individual_id):\n",
    "    num_layers = params['num_layers']\n",
    "    neurons_per_layer = params['neurons_per_layer']\n",
    "    activation_functions = params['activation_functions']\n",
    "    optimizer_name = params['optimizer']\n",
    "    alpha = params['alpha']\n",
    "    loss_function = params['loss_function']\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons_per_layer[0], activation=activation_functions[0], input_shape=(X_train.shape[1],)))\n",
    "    for i in range(1, num_layers):\n",
    "        model.add(Dense(neurons_per_layer[i], activation=activation_functions[i]))\n",
    "    model.add(Dense(1))  # Output layer\n",
    "    optimizer = get_optimizer(optimizer_name, alpha)\n",
    "    model.compile(optimizer=optimizer, loss=loss_function)\n",
    "    \n",
    "    history = model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0)\n",
    "    predictions = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    \n",
    "    # Save parameters to a .txt file\n",
    "    with open(f'individuals/individual_{individual_id}_params.txt', 'w') as file:\n",
    "        file.write(f'num_layers: {num_layers}\\n')\n",
    "        file.write(f'neurons_per_layer: {neurons_per_layer}\\n')\n",
    "        file.write(f'activation_functions: {activation_functions}\\n')\n",
    "        file.write(f'optimizer: {optimizer_name}\\n')\n",
    "        file.write(f'alpha: {alpha}\\n')\n",
    "        file.write(f'loss_function: {loss_function}\\n')\n",
    "        file.write(f'r2_score: {r2}\\n')\n",
    "    \n",
    "    # Save loss history to a .txt file\n",
    "    with open(f'individuals/individual_{individual_id}_loss_history.txt', 'w') as file:\n",
    "        for loss in history.history['loss']:\n",
    "            file.write(f'{loss}\\n')\n",
    "    \n",
    "    # Save y_test and y_pred to a .txt file\n",
    "    with open(f'individuals/individual_{individual_id}_predictions.txt', 'w') as file:\n",
    "        for index, true, pred in zip(list(y_test.index), y_test, predictions):\n",
    "            file.write(f'{index}, {true}, {pred[0]}\\n')\n",
    "    \n",
    "    return r2\n",
    "\n",
    "# Define the genetic algorithm operations\n",
    "def initialize_population(population_size):\n",
    "    population = []\n",
    "    for _ in range(population_size):\n",
    "        num_layers = np.random.choice(search_space['num_layers'])\n",
    "        neurons_per_layer = np.random.choice(search_space['neurons_per_layer'], num_layers).tolist()\n",
    "        activation_functions = np.random.choice(search_space['activation_functions'], num_layers).tolist()\n",
    "        optimizer = np.random.choice(search_space['optimizers'])\n",
    "        alpha = np.random.choice(search_space['alphas'])\n",
    "        loss_function = np.random.choice(search_space['loss_functions'])\n",
    "        chromosome = {\n",
    "            'num_layers': num_layers,\n",
    "            'neurons_per_layer': neurons_per_layer,\n",
    "            'activation_functions': activation_functions,\n",
    "            'optimizer': optimizer,\n",
    "            'alpha': alpha,\n",
    "            'loss_function': loss_function\n",
    "        }\n",
    "        population.append(chromosome)\n",
    "    return population\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    child = {}\n",
    "    child['num_layers'] = np.random.choice([parent1['num_layers'], parent2['num_layers']])\n",
    "    child['neurons_per_layer'] = [np.random.choice([p1, p2]) for p1, p2 in zip(parent1['neurons_per_layer'], parent2['neurons_per_layer'])]\n",
    "    child['activation_functions'] = [np.random.choice([p1, p2]) for p1, p2 in zip(parent1['activation_functions'], parent2['activation_functions'])]\n",
    "    child['optimizer'] = np.random.choice([parent1['optimizer'], parent2['optimizer']])\n",
    "    child['alpha'] = np.random.choice([parent1['alpha'], parent2['alpha']])\n",
    "    child['loss_function'] = np.random.choice([parent1['loss_function'], parent2['loss_function']])\n",
    "    # Adjust the lists to the number of layers\n",
    "    child['neurons_per_layer'] = child['neurons_per_layer'][:child['num_layers']]\n",
    "    child['activation_functions'] = child['activation_functions'][:child['num_layers']]\n",
    "    return child\n",
    "\n",
    "def mutate(chromosome):\n",
    "    key_to_mutate = np.random.choice(['num_layers', 'neurons_per_layer', 'activation_functions', 'optimizer', 'alpha', 'loss_function'])\n",
    "    if key_to_mutate == 'num_layers':\n",
    "        chromosome[key_to_mutate] = np.random.choice(search_space['num_layers'])\n",
    "        chromosome['neurons_per_layer'] = np.random.choice(search_space['neurons_per_layer'], chromosome[key_to_mutate]).tolist()\n",
    "        chromosome['activation_functions'] = np.random.choice(search_space['activation_functions'], chromosome[key_to_mutate]).tolist()\n",
    "    elif key_to_mutate == 'neurons_per_layer':\n",
    "        if chromosome['num_layers'] > 0:\n",
    "            layer_to_mutate = np.random.randint(0, chromosome['num_layers'])\n",
    "            chromosome[key_to_mutate][layer_to_mutate] = np.random.choice(search_space['neurons_per_layer'])\n",
    "    elif key_to_mutate == 'activation_functions':\n",
    "        if chromosome['num_layers'] > 0:\n",
    "            layer_to_mutate = np.random.randint(0, chromosome['num_layers'])\n",
    "            chromosome[key_to_mutate][layer_to_mutate] = np.random.choice(search_space['activation_functions'])\n",
    "    elif key_to_mutate == 'optimizer':\n",
    "        chromosome[key_to_mutate] = np.random.choice(search_space['optimizers'])\n",
    "    elif key_to_mutate == 'alpha':\n",
    "        chromosome[key_to_mutate] = np.random.choice(search_space['alphas'])\n",
    "    elif key_to_mutate == 'loss_function':\n",
    "        chromosome[key_to_mutate] = np.random.choice(search_space['loss_functions'])\n",
    "    \n",
    "    # Adjust the lists to the number of layers\n",
    "    chromosome['neurons_per_layer'] = chromosome['neurons_per_layer'][:chromosome['num_layers']]\n",
    "    chromosome['activation_functions'] = chromosome['activation_functions'][:chromosome['num_layers']]\n",
    "    \n",
    "    return chromosome\n",
    "\n",
    "\n",
    "\n",
    "def select_parents(population, num_parents):\n",
    "    sorted_population = sorted(population, key=lambda x: x['fitness'], reverse=True)\n",
    "    return sorted_population[:num_parents]\n",
    "\n",
    "# Define the genetic algorithm parameters\n",
    "population_size = 3\n",
    "num_generations = 3\n",
    "num_parents_to_select = 2\n",
    "\n",
    "# Initialize the population\n",
    "population = initialize_population(population_size)\n",
    "\n",
    "# Store parameters and R2 scores on each iteration\n",
    "results = []\n",
    "\n",
    "# Main loop of the genetic algorithm\n",
    "for generation in range(num_generations):\n",
    "    # Evaluate the fitness of each individual in the population\n",
    "    for i, individual in enumerate(population):\n",
    "        individual['fitness'] = evaluate_model(individual, f'g{generation}_i{i}')\n",
    "    \n",
    "    # Store parameters and R2 scores\n",
    "    for individual in population:\n",
    "        result = {\n",
    "            'num_layers': individual['num_layers'],\n",
    "            'neurons_per_layer': individual['neurons_per_layer'],\n",
    "            'activation_functions': individual['activation_functions'],\n",
    "            'optimizer': individual['optimizer'],\n",
    "            'alpha': individual['alpha'],\n",
    "            'loss_function': individual['loss_function'],\n",
    "            'R2_score': individual['fitness']\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    # Select the best individuals as parents for the next generation\n",
    "    parents = select_parents(population, num_parents_to_select)\n",
    "    \n",
    "    # Generate offspring through crossover and mutation\n",
    "    offspring = []\n",
    "    while len(offspring) < population_size:\n",
    "        parent1 = np.random.choice(parents)\n",
    "        parent2 = np.random.choice(parents)\n",
    "        child = crossover(parent1, parent2)\n",
    "        if np.random.rand() < 0.1:  # Mutation probability\n",
    "            child = mutate(child)\n",
    "        offspring.append(child)\n",
    "    \n",
    "    # Replace the old population with the offspring\n",
    "    population = offspring\n",
    "\n",
    "# Store results in a CSV file\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('genetic_algorithm_results_6.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
